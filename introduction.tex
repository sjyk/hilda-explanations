\section{Introduction}\label{intro}
Machine learning (ML) is increasingly an integral part of many software systems---from fraud detection, to content recognition, to automation.
This increased reliance on ML leads to the cruicial need to develop tools to debug and explain these models in order to understand failure cases and further improve their accuracy.
Traditional ML models were highly structured---decision trees are constrained to axis parallel splits---and their structure could be used to generate explanations for debugging.
In contrast, modern machine learning models such as deep neural networks~\cite{} learn their structure directly from the training data.
This results in uninterpretable, black-box models that render existing ML debugging tools ineffective.
While the database community has made substantial contributions in other areas of machine learning (e.g., training scalability and model specification~\cite{hellerstein2012madlib,tensor,kraska2013mlbase,crotty2014tupleware,keystone}), the opportunity is ripe for developing scalable tools to debug modern machine learning models.

Existing work on model debugging fall into two categories:
the first uses more structured models that are naturally easier to interpret, while the latter explains complex black-box models using a simplified approximation (a {\it surrogate model})~\cite{
taylor2016alignment, lei2016rationalizing, ribeiro2016should}. 
The former approach sacrifices prediction accuracy, which is a trade-off that many model applications are not willing to make.
\ewu{The latter approach is still a new field}.
One example is to identify a subset of the training data in the neighborhood of a mis-predicted test record, and fit an interpretable surrogate model to the subset of data~\cite{taylor2016alignment, lei2016rationalizing, ribeiro2016should}.
For instance, Ribeiro et al~\cite{ribeiro2016should} fit a sparse linear model and use the feature weights to explain the misprediction.

Although these approximations can identify salient features, deep neural networks are more reliant on the training data rather than features of the training records.
It may be more natural to help the developer answer questions such as:
  (1) what training data most influenced this prediction? (2) how different was this training data from the test example? and (3) where the differences were due to genuine variation or systematic errors in the training data?
Answering these questions can help the developer focus on cleaning the training dataset, gather more training data, or augmenting the model.


\if{0}
  This could potentially hide data representation inconsistencies between the training and test data.
  Identifying this training data is increasingly important when debugging deep neural network models due to their heavy reliance on large training datasets, which may not always be highly curated and consistent.


  \ewu{The (1), (2), (3) is actually the most useful bit of text.  the rest could probably be cut if they can be moved elsewhere}
  As a concrete example, consider designing a model for controlling the steering of a self-driving car.
  If, during simulation or testing, the models predicts that the car should turn into a sidewalk, the developer needs to quickly identify possible reasons for the misprediction.
  Due to its strong dependence on the training data, there are three natural questions that arise:
  Answering these questions can help the developer focus on cleaning the training dataset, gather more training data, or augmenting the model.
  %It is unlikely that there is an exhaustive set of training examples for every possible driving condition (e.g., during a once in five years storm).
  %Consequently, during deployment, the model will have to make predictions for new test data for which there are few similar examples in the training set.
\fi

\begin{figure}
    \centering
    \includegraphics[width=0.48\columnwidth]{figures/teaser1.png}
    \includegraphics[width=0.48\columnwidth]{figures/teaser2.png}
    \caption{(Left) ML developers need to understand the relationship between training data and a model's predictions, i.e., how are similar points predicted in the training data. But effectively determining the size and range of this neighborhood is challenging in expressive models which can have very complex geometry. (Right) We propose an approximation technique that can represent a black-box model in terms of sub-models applied to feature-space partitions. The developer can quickly determine which data are related based on these partitions.}
    \label{fig:my_label}
\end{figure}



For these reasons, we propose explaining mispredictions by identifying and summarizing the subset of training data that most influenced a mis-prediction.
%the missing piece is analogous to data provenance in traditional relational or data flow programs, where given an output tuple, the system describes the set of input tuples that cause it to be in the result.
%In the case of learning, our objective is to select a set of training data examples that most influence a prediction of a new example.
By influence, we mean that if the labels or features were perturbed for this subset, it would be more likely to change the prediction.
This notion is similar to responsibility in the data lineage and explanation literature~\cite{scorpion,alexandra,reversedatamanagamente,semirings}, in the context of black-box machine learning models as opposed to database queries with well defined semantics.

\ewu{This problem is challenging because every training data point has some impact on new prediction.}
Thus, we simplify the problem to instead identify a {\it sub-partition} of the training data locally approximates the complex model for test points within this sub-partition.  
High-dimensional, non-linear models, such as Neural Networks, have complex decision boundaries, which makes a simple nearest neighbor search unreliable.
Thus, we have to identify a subset of data that aligns with the decision structure of model.

We present \sysfull (\sys), which approximates a complex model (e.g., a deep neural network) using a two-part {\it surrogate model}: a meta-model that partitions the training data, and a set of sub-models that approximate the patterns within each partition.
These sub-models can be arbitrarily complex to capture intricate local patterns. However, the meta-model is constrained to be a decision tree.
This formulation ensures that sub-models are accurate, while the partitions are interpretable as rules.
In contrast to existing model interpretation techniques that return locally relevant features, our approach returns the most informative neighborhood (partition).

Applying \sys to a debugging task is very simple---if a test point is mispredicted, then we can identify the partition it belongs to and investigates that subset of the training data and its associated sub-model.
As our experiments show, the developer can tune the total number of sub-models and the depth of the decision tree to control the trade-off between the specificity of the partitions and how similar the surrogate model is to the complex model.
We anticipate that \sys is a starting point for the developer to further explore the training data and more quickly identify the problematic training data using other visualization or data summarization tools.

The rest of this short paper describes the \sys problem formulation, an algorithm sketch for training surrogate models, as well as an illustration of how \sys can be integrated into a visual model building interface.
Our experiments show that we can generate Fast And Clear Explanations using \sys.
We show that the explanations found by \sys select subsets of data that better align with the structure of the original model than the nearest neighbor or clustering baseline.
Furthermore, we show that \sys is nearly 30x faster than a nearest neighbor query, which is the next most accurate baseline.






