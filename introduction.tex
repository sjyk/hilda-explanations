\section{Introduction}\label{intro}
Machine learning is increasingly an integral part of many software systems for fraud detection, content recognition, automation, and control.
As this trend continues, the ability to debug and explain predictions made by machine learning models is crucial towards understanding failure cases and further improving their predictive accuracy.
For instance, if a model makes a bad prediction (e.g., drive into a wall), it is important to be able to understand the underlying reasons for usch a prediction---this can be in the form of features, input records, or rules used to make the prediction.
If the model is highly structured---for instance, a decision tree partitions the training dataset using axis parallel splits---then its structure can be used to generate explanations for faulty predictions.
In contrast, modern machine learning models such as deep neural networks effectivey learn a complex function over a massive training dataset using a black-box optimization process.
\ewu{say something about how it is turing complete}
While the database community has made substantial contributions in other areas of machine learning (e.g., training scalability and model specification~\cite{hellerstein2012madlib,tensor,kraska2013mlbase,crotty2014tupleware,keystone}), the opportunity is ripe for developing scalable tools to debug modern machine learning models.
\ewu{I would like the message to be: modern ML pretty much solely depends on the training data, so that is a good place to look if things go wrong}


% Due to the complexity of modern machine learning models such as neural networks, traditional database techniques such as lineage for root cause analysis and integrity constraints for detecting logical violations are not directly applicable.
% Whereas lineage can be used to identify the responsible input records that contributed to anamolous query results~\cite{} (perhaps identified by integrity constraints on the output view), model predictions typically depend on every record in the training dataset.  This renders traditional lineage-based debugging too imprecise to be effective.
% This challenge is exacerbated by the prevalence of increasingly opaque and high-dimensional models such as Neural Networks, where even considering nearest neighbors might be misleading.
% While the database community has made substantial progress in training scalability and model specification~\cite{hellerstein2012madlib,tensor, kraska2013mlbase, crotty2014tupleware, keystone}, the lack of a suitable machine learning analog of data provenance is a critical bottleneck in the development of debugging tools.

\ewu{Exsiting work using model structure.  Recent work suggests surrogate models by using nearest neighbor. They find features, but not the relevant data. }
Existing work proposes techniques to enhance model interpretability as one possible solution.
Interpretability can be achieved in two ways: (1) use a simpler, more interpretable model (e.g., a decision tree) from the start, or (2) explain the behavior of a complex model using a simplified approximation~\cite{taylor2016alignment, lei2016rationalizing, ribeiro2016should}. Neither approach is perfect. 
Using a simpler model sacrifices prediction accuracy, which is a trade-off that many model applications are not willing to make.
An example of the second approach is to train a sparse linear model in the local neighborhood of a mispredicted point as a proxy to explain the characteristics (features) that caused the misprediction\cite{taylor2016alignment, lei2016rationalizing, ribeiro2016should}.
Although these approximations can identify salient features, they do not help identify the {\it training data} that led the model towards the misprediction, which might be more useful to a model developer.
This could potentially hide data representation inconsistencies between the training and test data.
Identifying this training data is increasingly important when debugging deep neural network models due to their heavy reliance on large training datasets, which may not always be highly curated and consistent.


\ewu{Why is finding the training data important?  Highligt via an example.}
As a concrete example, consider designing a model for controlling the steering of a self-driving car.
If, during simulation or testing, the models predicts that the car should turn into a sidewalk, the developer needs to quickly identify possible reasons for the misprediction.
Due to its strong dependence on the training data, there are three natural questions that arise:
(1) what training data most influenced this prediction? (2) how different was this training data from the test example? and (3) where the differences were due to genuine variation or systematic errors in the training data?
Answering these questions can help the developer focus on cleaning the training dataset, gather more training data, or augmenting the model.
%It is unlikely that there is an exhaustive set of training examples for every possible driving condition (e.g., during a once in five years storm).
%Consequently, during deployment, the model will have to make predictions for new test data for which there are few similar examples in the training set.

\begin{figure}
    \centering
    \includegraphics[width=0.48\columnwidth]{figures/teaser1.png}
    \includegraphics[width=0.48\columnwidth]{figures/teaser2.png}
    \caption{(Left) ML developers need to understand the relationship between training data and a model's predictions, i.e., how are similar points predicted in the training data. But effectively determining the size and range of this neighborhood is challenging in expressive models which can have very complex geometry. (Right) We propose an approximation technique that can represent a black-box model in terms of sub-models applied to feature-space partitions. The developer can quickly determine which data are related based on these partitions.}
    \label{fig:my_label}
\end{figure}



\ewu{From this example, we argue for explaining model mispredictions by identifying and summarizing the subset of training data that most influenced the prediction of a test record.}
%the missing piece is analogous to data provenance in traditional relational or data flow programs, where given an output tuple, the system describes the set of input tuples that cause it to be in the result.
%In the case of learning, our objective is to select a set of training data examples that most influence a prediction of a new example.
By influence, we mean that if the labels or features were perturbed for this subset, it would be more likely to change the prediction.
This notion is similar to responsibility in the data lineage and explanation literature~\cite{scorpion,alexandra,reversedatamanagamente,semirings}, in the context of black-box machine learning models as opposed to database queries with well defined semantics.

\ewu{This problem is challenging because every training data point has some impact on new prediction.}
Thus, we simplify the problem to instead identify a {\it sub-partition} of the training data locally approximates the complex model for test points within this sub-partition.  
High-dimensional, non-linear models, such as Neural Networks, have complex decision boundaries, which makes a simple nearest neighbor search unreliable.
Thus, we have to identify a subset of data that aligns with the decision structure of model.

We present \sysfull (\sys), which approximates a complex model (e.g., a deep neural network) using a two-part {\it surrogate model}: a meta-model that partitions the training data, and a set of sub-models that approximate the patterns within each partition.
These sub-models can be arbitrarily complex to capture intricate local patterns. However, the meta-model is constrained to be a decision tree.
This formulation ensures that sub-models are accurate, while the partitions are interpretable as rules.
In contrast to existing model interpretation techniques that return locally relevant features, our approach returns the most informative neighborhood (partition).

Applying \sys to a debugging task is very simple---if a test point is mispredicted, then we can identify the partition it belongs to and investigates that subset of the training data and its associated sub-model.
As our experiments show, the developer can tune the total number of sub-models and the depth of the decision tree to control the trade-off between the specificity of the partitions and how similar the surrogate model is to the complex model.
We anticipate that \sys is a starting point for the developer to further explore the training data and more quickly identify the problematic training data using other visualization or data summarization tools.

The rest of this short paper describes the \sys problem formulation, an algorithm sketch for training surrogate models, as well as an illustration of how \sys can be integrated into a visual model building interface.
Our experiments show that we can generate Fast And Clear Explanations using \sys.
We show that the explanations found by \sys select subsets of data that better align with the structure of the original model than the nearest neighbor or clustering baseline.
Furthermore, we show that \sys is nearly 30x faster than a nearest neighbor query, which is the next most accurate baseline.






